\subsection{Gradient methods}
In the following, we assume $H$ to be Hilbert space.
\paragraph{Motivation}
	Dynamical systems in physics are often based on the idea of gradient flow with respect to the energy which the system follows. Consider $f$ $g$ the heat equation, then, the thermal energy is given by
	\[
		E(u)=\frac{1}{2}\int \abs{\nabla u}^2 dx.
	\]
	
	The gradient flow is defined by,
	\[
		\frac{\partial u}{\partial t} = -E(u),
	\]
	which yields the heat equation.
	\[
		\dot{u}(t)=\Delta u
	\]
In order to obtain a minimization method in $U$, we introduce the gradient flow $\frac{\partial u}{\partial t} = -J'(u)$, with $J'(u)\in H$ which can be associated with the gradient $J$ at $u$, i.e.

\[
	\left\langle \pdev{u}{t}, v \right\rangle = \left\langle-J'(u), v\right\rangle = -\mathsf{D}J(u;v), \quad \forall v \in H
\]

The evolution of $J$ corresponding to the gradient flow is given by

\[
	\frac{\partial}{\partial t} \left(J(u)\right) = \left\langle J'(u), \pdev{u}{t} \right\rangle = -\norm{\pdev{u}{t}}^2 \leq 0
\]

the objective function is decreasing and $\pdev{}{t} J(u) =0$ if and only if $\pdev{u}{t}=0$.

Consequently, the gradient flow will decrease the objective function $J$ until the evolution reaches a stationary point. In order to obtain an .... optimization method, we use an explicit time discretization of the flow, i.e.
\begin{align*}
u_{k+1}&=u_k - \tau_kJ'(u_k) \quad \forall k=0,1,2,.. \\
J'(u_{k+1})&=0
\end{align*}
with appropriate (small) choice of the time step $\tau_k >0$. We are only ...... in the minimization of the objective function and not in the accurate approximation of the solution of the gradient flow.

Therefore we select the step size ... based on the suitable .... of objective function. A classical way to do this is the so called .......... methods, which are based on the "effective descent"

\[
\mathcal{D}_{\text{eff}}(\tau) = J(u_k+\tau s)-J(u_k)
\]
and the "expected descent"
\[
	\mathcal{D}_{\text{exp}} = \tau J'(u_k)s
\]
where  $s=-J'(u_k)$. They are related to each other by the Taylor formula
\[
	\mathcal{D}_{\text{eff}}(\tau)-\mathcal{D}_{\text{exp}}(\tau)=O(\tau)
\]
Therefore, we can test if 
\begin{equation}
\alpha \mathcal{D}_{\text{exp}}(\tau)\leq \mathcal{D}_{\text{eff}}(\tau) \leq \beta \mathcal{D}_{\text{exp}} \label{eq10. DexpDeffDexp} \tag{*}
\end{equation}

with constants $0<\beta<\alpha<1$. If $\mathcal{D}_{\text{eff}} > \beta \mathcal{D}_{\text{exp}}$, then $\tau$ is too large and we decrease it. 
If $\mathcal{D}_{\text{eff}}<\alpha\mathcal{D}_{\text{exp}}$, we could still increase $\tau$. We accept $\tau$, if 
\eqref{eq10. DexpDeffDexp} is fulfilled. Typical choices of the constants are $\alpha \approx 0.9$ and $\beta \approx 0.1$.

Moreover, the ..... for increasing and decreasing $\tau$ should be different, i.e. multiplying with $1.5$, for increasing and dividing by $2$ for decreasing.

\begin{theorem}
	Let $J:H\rightarrow \overline{\mathbb{R}}$ be twice Fr\'echet-differentiable and weakly lower semicontinuous on a Hilbert space $U$. Moreover, let the level sets,
	\[
		\mu_\xi = \{ u \in U \mid J(u)\leq \xi \}
	\]
	be bounded in $U$ for each $\xi \in \mathbb{R}$ and empty for sufficiently small $\xi$. Then the sequence $(u_k)_k$ generated by the gradient method with the Amijo-Goldstein line search has a weakly convergent subsequence, whose limit is a stationary point. 
	
	\begin{proof}
		Since the gradient method is a descent method we have $J(u_k) \leq J(u_0)$, $\forall k \geq 0$,
		i.e. $(u_k)_k$ is bounded, implies corollary ... $\exists (u_k)_k \xrightharpoonup[l\rightarrow\infty]{} \overline{u}$.
		
		\begin{align*}
			\sum_{k=0}^N \norm{u_{k+1}-u_k}^2 &= \sum_{k=0}^N \langle -\tau_k J'(u_k), u_{k+1}-u_{k}\rangle \\
			&\frac{1}{\beta} \mathcal{D}_{\text{eff}}(\tau)=\frac{1}{B}(J(u_k+\tau s)-J(u_k)) \leq \mathcal{D}_{\text{exp}}(\tau)=\tau J'(u_k)s\\
			&\leq \frac{1}{\beta} \sum_{k=0}^{N} (J(u_k)-J(u_{k+1}) \\&= \frac{1}{\beta} (J(u_k)-J(u_{k+1})) \\
			&\leq \frac{1}{\beta} (J(u_0)-\inf_{u\in U} J(u)) = q
		\end{align*}
	$q$ is .... of $N$; for $N\rightarrow \infty$ we obtain \[
	\sum_{l=0}^{\infty} \norm{u_{k+1}-u_{k}}^2
	\]
	
	Thus, there is a subsequence $(u_k)_l$, such that
	
	\[
	\norm{\tau_{k_l}J'(u_{k_l})}=\norm{u_{{k_l}+1}-u_{k_l}} \xrightarrow{l\rightarrow \infty} 0.
	\]
	
	$\exists c <0$, $J''(u_{k_l})(v,v)\leq c \norm{v}^2$, $\forall v \in U$.
	
	Amijo-Goldstein implies,
	
	\begin{align*}
		\alpha \mathcal{D}_{\text{exp}} (\tau) = \alpha J'(u_{k_l};u_{{k_l}+1}-u_{{k_l}}) &\leq \alpha \mathcal{D}_{\text{exp}}(\tau_{k_l})= J(u_{k_l+1})-J(u_{k_l}) \\
		&= J'(u_{k_l};u_{k_l+1}-u_{k_l}) + \int_{0}^{1} J''(u_{k_l}+t(u_{k_l+1}-u_{k_l})) (u_{k_l+1}-u_{k_l}, u_{k_l+1}-u_{k_l}) dt \\
		& \leq J'(u_{k_l}) (u_{k_l+1}-u_{k_l}) + c \norm{u_{k_l+1}-u_{k_l}}^2
		\end{align*}
	implies,
	
	\begin{align*}
	(\alpha -1) J'(u_{k_l})(u_{k_l+1}-u_{k_l}) &\leq c \norm{u_{k_l+1}-u_{k_l}}^2 
	\end{align*}
	Since $u_{k_l+1}-u_{k_l}= -\tau_k J'(u_{k_l})$:
	
	\begin{align*}
		(1-\alpha) \tau_{k_l} \norm{J'(u_{k_l})}^2 \leq c \tau_{k_l}^2\norm{J'(u_{k_l})}^2
	\end{align*}
	
	$J'(u_{k_l})=0$, or $ 1-\alpha \leq c \tau_{k_l}$. But since $1-\alpha > 0$ and $c <0$. We have
	
	$J'(u_{k_l}) =0$, implying we r
	
	\[\implies \norm{J'(u_{k_l})} \rightarrow 0 \implies J'(\overline{u})=0\].
	
	Implies Algorithm reached stationary point $u_j= u_{k_l} \forall j \geq k_l$
	\end{proof}
\end{theorem}
